{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleansing\n",
    "\n",
    "Sections:\n",
    "* Data Cleansing\n",
    "* Data Anomalies\n",
    "* Data Quality \n",
    "* Steps of Data Cleansing\n",
    "\n",
    "<br>\n",
    "\n",
    "Note most of this lecture follows closely from _Müller, H., & Freytag, J. C. (2005). Problems, methods, and challenges in comprehensive data cleansing. Professoren des Inst. Für Informatik._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Data Cleansing\n",
    "\n",
    "<br>\n",
    "Real world data is often messy. Really messy. Rife with errors and noise. How do you identify errors? How to you remove or account for them? \n",
    "\n",
    "That is the fundamental goal of _data cleansing_.\n",
    "\n",
    "__Data Cleansing:__ The identification and accounting for anomalies in your data.\n",
    "\n",
    "Remember that, ideally, your data is a veridical mapping of some real world phenomenon onto a specific set of obsrvations. Breaks or errors in that mapping can dramatically impact how you indirectly observe the world. These breaks are called _anomalies_.\n",
    "\n",
    "__Anomaly:__ A property of data values that renders them an incorrect representation of the world. Let's consider different types of data anomalies you might encounter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---\n",
    "# 2. Types of anomalies\n",
    "\n",
    "<br>\n",
    "In their chapter, Müller & Freytag (2005) identify 3 classes of data anomalies. But before we go into them, we should make sure that our definitions of terms is up to date. For the discussion below we will use the following defintions.\n",
    "\n",
    "__Data:__ symbolic representations of information, i.e., facts or entities from the world, depicted by symbolic values.\n",
    "\n",
    "__Tuple:__ sets of related discrete values from a finite set of domains (i.e., another term for a variable).\n",
    "\n",
    "__Feature vector:__ A collection of observations from the same variable.\n",
    "\n",
    "Now that we have those out of the way, let's go over the different classes of data anomalies.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### __A. Syntactical Anomalies__\n",
    "\n",
    "Syntactical anomalies are errors in the formatting of data labels or formats. This can be at the level of full feature vectors or an individual tuple. There are several types of syntactical anomalies. \n",
    "\n",
    "<br>\n",
    "\n",
    "__i. Lexical errors:__ Discrepancies between the structure of the data item and the specified format. This can happen when there's a misalignment between observations & variables (see Lecture 2 on TidyData). Consider this example:\n",
    "\n",
    "| Trial | Condition | RT  | Accuracy\n",
    "| ------|:---------:|----:|----:|\n",
    "| 1     |     A     | 380 |  I\n",
    "| 2     |     B     | 599 |  C\n",
    "| 3     |     A     | C   |  \n",
    "\n",
    "Notice that because of a formatting error, the variable RT and Accuracy (C=correct, I=incorrect) have bled over into one another. This results in not only messy data, but requires examining how to reconfigure the data in the case of missing variables.\n",
    "\n",
    "<br>\n",
    "\n",
    "__ii. Domain format errors:__ Errors where the given value for an attribute (A) does not match the anticipated domain format (G(dom(A))). An easy one is with names as string objects. Consider the Name column in this table.\n",
    "\n",
    "|    Name    | Condition | Score  | \n",
    "| -----------|:---------:|-------:|\n",
    "| Smith, John|     A     |  380   |\n",
    "| Doe, Jain  |     B     |  599   |\n",
    "| Pain Max   |     A     |  1000  |  \n",
    "\n",
    "Notice that the last name \"Pain Max\" is missing the key comma that is required for the format of \"LAST, FIRST\" that is assumed in the other entries of the table.\n",
    "\n",
    "<br>\n",
    "\n",
    "__iii. Irregularities:__ The non-uniform use of values, units, or abbreviations. For example, look at the RT variable in this data table.\n",
    "\n",
    "| Subject | Trial | Condition | RT    | Accuracy\n",
    "| --------| ------|:---------:|--------:|----:|\n",
    "| s0001   | 1     |     A     | 380     |  0\n",
    "| s0001   | 2     |     B     | 0.599   |  1\n",
    "| s0001   | 3     |     A     | 2.4 x 10^2|  1\n",
    "| s0002   | 1     |     B     | 0.692   |  0\n",
    "| s0002   | 2     |     B     | 476   |  1\n",
    "| s0002   | 3     |     A     | 301   |  1\n",
    "\n",
    "Here the format of the RT changes from milliseconds (first row), to seconds (second row), to a scientific notation format (third row). So the entries are all in an inconsistent format. One place this really gets hairy is when marking down things like date of birth. For example using “4/14/1978” for one subject and “March 17, 1983” for another. \n",
    "\n",
    "These irregularity anomalies can be sneaky. For example, in the table above, if you don't pay attention to format regularities, your data analysis might assume that some subjects have sub-millisecond speeds, while others remain in the hundreds of milliseconds speed.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### __B. Semantic Anomalies__\n",
    "\n",
    "Semantic anaomalies are errors in the fundamental value of observations themselves, rather than their specific formatting. Let's consider the different types of semantic anomalies.\n",
    "\n",
    "<br>\n",
    "\n",
    "__i. Integrity constraint violations:__ When data values of observations do not match the constraint of the attribute. Many of the variables we collect have natural constraints. For example, things like age, firing rates, and height cannot be less than zero. Thus zero is a natural constraint on these variables. So an error in entry that places a value outside of these constraints (e.g., AGE = -1) is an integirty constraint violation.\n",
    "\n",
    "<br>\n",
    "\n",
    "__ii. Contradictions:__ When values within a feature or between features violate a known dependency. Unlike integrity constraint violations, where an external assumption is violated in a single variable, contradictions are cases where a meaningful relationship between two variables is broken. A simple example of this is when the listed age and date of birth of a subject do not match up. The assumption is that if you know one (date of birth) you can directly infer the other (age at time of testing). If there is a discrepency that violates this relationship, then you have yourself a good ol' contradiction. \n",
    "\n",
    "<br>\n",
    "\n",
    "__iii. Duplicates:__ When two or more data points represent the same thing. This is sort of the inverse problem of contradictions. Rather than dependencies between variables being violated, duplication errors are when too many variables are identical across observations. For example, look at this data table.\n",
    "\n",
    "|    Name    |    DOB    | Sex  | Height |\n",
    "| -----------|:---------:|-----:|-------:|\n",
    "| Smith, John| 4/14/78   |  M   | 72     |\n",
    "| Doe, Jan   | 1/3/42    |  F   | 61     |\n",
    "| Pain,Max   | 4/14/78   |  M   | 72     |\n",
    "\n",
    "Notice that John Smith and Max Pain have the same height and date of birth (DOB). Thus it is possible that there is a redundancy in these entries that might mean one subject is a duplicate of the other.\n",
    "\n",
    "<br>\n",
    "\n",
    "__iv. Invalid tuples:__ Data points or sets of data points that do not fit the other semantic or syntactic errors but nonetheless represent invalid representations of the real world. This is sort of a catch all of errors. Often we call these values \"outliers\" or \"extreme values\". Here it is more a matter of assumptions and expectations of the data itself. For example, while it is possible (numerically speaking) to have an age of 132 years old (it wouldn't violate the syntactical or sematic constraints), it is very likely that that tuple is an error. \n",
    "\n",
    "<br>\n",
    "\n",
    "#### __C. Coverage Anomalies__\n",
    "\n",
    "The final type of error you may encounter in your data tables are coverage errors. Here it's more a reflection of gaps in the data collection process itself.\n",
    "\n",
    "<br>\n",
    "\n",
    "__i. Missing values:__ Omissions of data within a tuple/variable. For example during data collection, a research assistant forgot to enter in a subject's date of birth. The value is simply missing, but the rest of the set may be mostly intact. \n",
    "\n",
    "<br>\n",
    "\n",
    "__ii. Missing tuples:__ The complete omission of a relevant feature vector. For example, that same research assistant forgot to enter in an entire variable. This is the toughest error to identify because, from a data science perspective, there's no way to easily know when this has happened and not usually an easy way to infer these values from other variables (although if there is a clear dependency, like date of birth and age, then you can). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Assessing data quality\n",
    "\n",
    "<br>\n",
    "\n",
    "Now that you know what kind of mistakes to look for, how do you go about cleaning things up. Remember Rule 2 from the first lecture: If you're doing things by hand, then you're doing them wrong. You want to ideally develop automated steps for identifying and correcting anomalies in your data. But to do that you want to establish a process.  Müller & Freytag propose a set of data quality criteria that are impacted by the data anomalies.\n",
    "\n",
    "<br>\n",
    "\n",
    "![Process of data cleansing](imgs/L5Hierarchy.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "These hierarchically organized criteria are meant to be the targets of a data cleansing pipeline. Therefore they are useful to understand how to approach the _process_ of data cleansing, rather than define a specific process itself. Consider these more philosophies rather than \n",
    "\n",
    "<br>\n",
    "\n",
    "#### __I. Accuracy:__\n",
    "\n",
    "Are your data exact, uniform, and complete representations of the world? You can measure accuracy by evaluating three classes of criteria.\n",
    "\n",
    "<br>\n",
    "\n",
    "__A. Integrity:__ This deals with mostly _semantic_ anomalies. Evaluating whether a data set contains representations of all the desired aspects of the real world and only those aspects. There are two specific steps you can take to evaluate data integrity: evaluating completeness & the validity of your measures.\n",
    "\n",
    "* __i. Completeness:__ Making sure you have all the variables that you should have. In other words, do you have a complete representation of the data collected? Note that completeness is why we seek to _account_ for anomalies rather than remove them.\n",
    "\n",
    "* __ii. Validity:__ Making sure your data does not have contradictions or invalid tuples. For example, account for outliers.\n",
    "\n",
    "<br>\n",
    "\n",
    "__B. Consistency:__  This deals mostly with _syntactical_ anomalies, such that your data is syntactically uniform and free of contradictions. Just like  integrity, evaluating consistency has two specific steps.\n",
    "\n",
    "* __i. Schema conformance:__ This means ensuring that all values in your dataset conform to the formatting schema of your data entry. Are all the dates of birth in the same format? Are the character strings following the same rules?\n",
    "\n",
    "* __ii. Uniformity:__ Determining the proportion of values in your dataset that fall within the specific variable ranges specified by your schema. For example do you have negative ages or reaction times?\n",
    "\n",
    "<br>\n",
    "\n",
    "__C. Density:__ When the total number of entries in your dataset (both variables and data points) matches the expectation you had from your experimental design. Specifically, if you have $n$ observations & $p$ variables, then you should have exactly $n * p$ data points. Doing checks for data density allows for you to identify both lexical errors and missing values.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### __II. Uniqueness:__\n",
    "\n",
    "Evaluations of uniqueness look at the quotient of entries in your dataset that represent the same entity in your sample. Do you have duplicate observations? Do you have two versions of the same variable?\n",
    "\n",
    "<br>\n",
    "\n",
    "___\n",
    "\n",
    "<br>\n",
    "\n",
    "As you can see, each data evaluatation criteria targets at least one, if not a set of, anomaly types. Müller & Freytag go on to highlight _how_ each anomaly type can impact the data quality criteria. \n",
    "\n",
    "<br>\n",
    "\n",
    "![Criteria and Anomalies](imgs/L5DataCriteriaAndAnomalies.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "Here the ($\\bullet$) show when an anomaly directly impacts the data quality criteria, where ase the (-) indicates where the presence of anomaly can impact the ability to detect other anomalies that can impact the data collection criteria.\n",
    "\n",
    "When presented this way, you can see how different anomalies can impact differnet data quality criteria. The goal of any data cleaning pipeline is to _target the anomalies in a strategic manner so as to improve specific data quality criteria_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Steps of data cleansing\n",
    "\n",
    "In the previous section, Müller & Freytag showed how different data anomalies can impact specific data quality criteria and how this gives you insights into building a data cleansing pipeline. Now we will go over their philosophy for how to build such pipelines.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### __Necessary steps of a data cleansing pipeline__\n",
    "\n",
    "Müller & Freytag suggested this iterative approach to data cleansing.\n",
    "\n",
    "<br>\n",
    "\n",
    "![Data cleansing pipeline](imgs/L5Pipeline.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "Let's now go over what each of these steps are.\n",
    "\n",
    "<br>\n",
    "\n",
    "__Step 1: Data auditing.__ This involves taking a subset of the data (if you're working with a large data set) to search for data anomalies. It assumes that anomalies are evenly distributed throughout your data table. Therefore if you take random subsets of the data and identify anomalies in them, you know what type of corrections you'll need to do on the entire data set. For example, if you take say the first 100 subjects from a data table with 10,000 subjects, and notice that the formatting of each subject's data of birth is different (e.g., \"April 14th, 1978\", \"3/17/1983\"), then you know you have to deal with irregular syntactical anomalies in that variable. \n",
    "\n",
    "<br>\n",
    "\n",
    "__Step 2: Workflow specification.__ Once you've identified the types of anomalies you need to correct for, specify a set of procedures for how to detect and deal with anomalies. All data cleansing workflows have two stages:\n",
    "\n",
    "* __Detection:__ Scripts that can classify consistency in the data and identify anomalies\n",
    "* __Resolution:__ Systematic algorithms/logic for how to deal with each anomaly type.\n",
    "\n",
    "<br>\n",
    "\n",
    "__Step 3: Workflow execution.__ Once you know what anomalies you need to correct for and once you have a workflow setup where you will correct for these errors, execute the workflow (seems like a \"no brainer\" but they're just trying to be thorough here).\n",
    "\n",
    "<br>\n",
    "\n",
    "__Step 4: Post-processing control.__ Presumably if you executed your workflow correctly, then you shouldn't have any more anomalies. But don't rest on that assumption. Re-audit the data and validate validate the steps in the workflow you executed. If no more anomalies are detected, then stop. If anomalies persist (or new ones emerged), then you'll need to correct for those and run Steps 2-4 again.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### __Don't touch the original data__\n",
    "\n",
    "These steps laid out by Müller & Freytag define a logic to how to setup data cleansing steps. However, there is one applied step that you should consider. Whatever the raw data is, you _always want to keep a back up file of the raw (and error filled) data file_. This is cruicial as there is always a chance that your data cleansing workflow could cause more anomalies than it fixes. Sometimes it is eaiser to start from scratch. So always have a raw data file (that may have errors in the data table) and a cleaned data file, that has gone through your workflow. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
