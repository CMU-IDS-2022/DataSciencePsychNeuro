{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic classifiers\n",
    "\n",
    "- Logistic regression\n",
    "- LDA\n",
    "- QDA\n",
    "\n",
    "The lecture draws from Chapter 4 of James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). \"An introduction to statistical learning: with applications in r.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Application**: a categorical response variable - recall accuracy, disease diagnosis, etc\n",
    "\n",
    "**Question**: Why not linear regression?\n",
    "- if you code categories with numbers, e.g. 1, 2, 3, you *could* run a linear regression, but the results will be uninterpretable - there is no particular reason why one category is assigned the number 3, rather than 2, but linear regression assumes that 3 is not only more than 2, but that the difference between 2 and 3 is the same as that between 1 and 2. For example, if you are trying to determine which disease is consistent with a set of symptoms it does not make sense to say that disease A is more than disease B\n",
    "- for a binary case, for example, correct vs incorrect recall, you could code corrects as 1 and incorrect as 0 and run a linear regression to estimate the probability of recall. However, your linear model, as in the figure below might predict probability less than 0 or more than 1, which is impossible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![linear regression on binary data](imgs/L12Fig1a.png)\n",
    "\n",
    "**[*Note*:** since ANOVA is just a special case of linear regression, it is also inappropriate for analyzing proportion data, although this is a very common practice]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Logistic Regression model \n",
    "*(supervised method, parametric, classification)*\n",
    "\n",
    "Let's focus on the binary case, because it is the most common one. Consider the stop-signal task paradigm, in which participants have to identify a stimulus with a button press, but some of the stimuli are followed by a \"stop signal\" which instructs participants to withold response. This stop signal occurs after variable intervals, and we would like to figure out the probability of succesfull stopping for a given interval. \n",
    "\n",
    "If we apply a linear regression model, as in the figure above, we do see that it generally captures the fact that stopping is more likely as the interval increases. However, it predicts negative probability for fast intervals and at maximum it predicts ~40% probability even for the slowest speeds, for which we can see that the data shows alsmost exclusively succesfull stopping. \n",
    "\n",
    "What we need in this case is a model function that would make predictions bound between 0 and 1. The most common one is the logistic or binomial function shown on the figure below:\n",
    "\n",
    "![linear regression on binary data](imgs/L12Fig1b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we are not modelling Y ~ f(X), as we are doing with linear regression. Rather, we are estimating the probability of Y being a specific category, depending on the value of X or:\n",
    "\n",
    "$$P(Y = 1  |  X)  \\sim f(X)$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we are really modeling with a logistic regression model is a **state transition function**, which is a function that describes a rapid (in this case exponential) change in state. Specifically, we model this transition using the logistic function, which has this form\n",
    "\n",
    "$$ p(X) = \\frac{e^{\\beta_0+\\beta_1X}}{1+e^{\\beta_0+\\beta_1X}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than estimating P(Y=1|X) directly, however, we could rewrite this function it terms of the __odds__ of Y being a specific category.\n",
    "\n",
    "$$odds(Y=1) = \\frac{p(X)}{1-p(X)} = e^{\\beta_0+\\beta_1X}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we could take the logarithm of the odds, known as the log odds, and we get a function similar to that of linear regression.\n",
    "\n",
    "$$\\log\\bigg(\\frac{p(X)}{1-p(X)}\\bigg) = \\beta_0+\\beta_1X$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The odds ratio ranges from 0 to Inf, and the log odds range between -Inf and +Inf, so in this case they could be a linear function of X without the issues outlined above. Notice the simplicity here. You can still think of this as a linear problem, but instead of Y changing in a slow, smooth fashion with changes in X, it has a rapid change for a certain subset of values of X.\n",
    "\n",
    "This is meaningful when trying to understand what factors in X relate to Y. However, interpreting the nature of this relationship can be a bit more difficult with logistic regersion. \n",
    "\n",
    "Why?\n",
    "\n",
    "Remember, for linear regression, a beta value of 0.5 means that one unit change in X is associated with 0.5 units change in Y. For logistic regression the interpretation is not as direct - one unit change in X is a 0.5 unit change in the log odds of Y being equal to 1.\n",
    "\n",
    "There is no straightforward way to transform this into unit changes in p(X) directly. Why, after all we have a formula, right? The problem is that addition turns into multiplication in exponential space. E.g.:\n",
    "\n",
    "$$e^{\\beta_0+\\beta_1X} = e^{\\beta_0}e^{\\beta_1X}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, how much $P(Y=1|X)$ will change as one predictor in X increases by one unit will depend on the values of the other predictors. You can also see that in the S-shape form of the function above. \n",
    "\n",
    "*In general, however, you can interpret the direction and relative size of coefficients the same way you do in linear regression* - positive coefficients means that p(X) increases as X increases, and bigger beta values imply a bigger increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Classification threshold\n",
    "\n",
    "If you look carefully, you'll notice that while we are trying to classify a binary state of Y (either 0 or 1), we are modeling Y as a continuous value *between* 0 and 1. In most cases, to make a determination of what state Y is in from X, we have to also add in a *classification threshold*. Which is a threshold of Y that determines whether you think a particular value of X belongs to Y=1 or Y=10. \n",
    "\n",
    "The default is to assume that the threshold is Y=0.5, which is interpreted as there being a greater or lesser than 50% chance that Y is 1. Although, particularitys in a data set might introduce biases where a 50% chance threshold is not realistic. \n",
    "\n",
    "Therefore, to make a prediction of the state of Y, you need two thingss. \n",
    "\n",
    "1. Prediction of Y from $\\hat{f}(X)$\n",
    "2. A threshold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Estimating parameters\n",
    "\n",
    "In contrast to linear regression, logistic regression does not have a closed form solution like least squares. Logistic regression paramters have to be estimated with maximum likelihood techniques. Conceptually, the goal is to find $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$ such that plotting $\\hat{y}$ is close to 1 for all values where $y=1$ and close to zero for all values where $y = 0$. \n",
    "\n",
    "The **likelihood function** that has to be optimized is:\n",
    "\n",
    "![Likelihood function](imgs/L12Fig2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood will be highest when the probabilities of the corresponding values of X for y=1 and of X for y=0 are highest under the corresponding beta values.Thus, this respresents the likelihood of these coefficients given the model and parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Multiple predictors\n",
    "\n",
    "The logistic regression model is nice as it expands out to multiple predictor variables quite easily, in the same way as linear regression:\n",
    "\n",
    "$$ \\log\\bigg(\\frac{p(X)}{1-p(X)}\\bigg) = \\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p$$\n",
    "\n",
    "$$ p(X) = \\frac{e^{\\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p}}{1+e^{\\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p}}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "\n",
    "- y = probability of getting the flu\n",
    "- x1 = age\n",
    "- x2 = occupation (1 if teacher, 0 otherwise)\n",
    "- x3 = social network size\n",
    "\n",
    "If I model the probability of getting the flu as a function of age, occupation, and social network size, I can infer the significance of each regression coefficient exactly the same now as linear regression:\n",
    "$$t=\\frac{\\hat{\\beta}}{SE(\\hat{\\beta})}$$\n",
    "\n",
    "Thus if I get signiificant (i.e., $p<\\alpha$) coefficients for age and social network size, I can infer that both factors contribute to someone getting the flu.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When is logistic regression not the most suitable method?\n",
    "\n",
    "Logistic regression works well for many simple cases, however, it is still limited in certain cases. In particular, logistic regression does not work when:\n",
    "\n",
    "- Y has more than 2 classes\n",
    "- If n is small parameter estimates are sometimes less stable than other methods\n",
    "- When the classes are too well separated (i.e., a big distance between the observations of X when Y = 0 and when Y = 1).\n",
    "\n",
    "The first two limitations seem intuitive. Afterall, by definition Y can only be between 0 and 1 (or to be more accurate, you are only testing the probability that Y = 1 or not). Also, the nature of the exponential change from one state to another would intuitively make it seem that parameter estimates are not stable in that transition boundary.\n",
    "\n",
    "But, why does logistic regression fail when the separation along X is too good? Well think about what we are modeling. We are trying to find the range of where Y transitions between two states. If there is a big gap in the distributions of X, then there can be a lot of possible shapes to the logistic function. This leads to high uncertainty as to the true shape of the logisitic curve.\n",
    "\n",
    "When logistic regression can't be used due to these reasons, we can use several other methods. In the next section we'll focus on Linear discriminant analysis (LDA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Linear discriminant analysis (LDA) \n",
    "*(unsupervised method, parametric, classification)*\n",
    "\n",
    "In contrast to logistic regression, that models P(Y=1|X) as a logistic function f(X), LDA models the distribution of X scores separately for each category of Y. Thus, it obtains the _probability distribution_ of X given a specific value of Y, $p(X | Y =k)$. It then uses Bayes theorem to arrive at the desired probability, namely the probability of classifying Y to a category k given a specific value of the predictor X, $P(Y=k|X)$.\n",
    "\n",
    "It is interesting to note that when distributions are normal and Y only has 2 groups, this becomes a special case of logistic regression. So in a way, LDA is a more powerful generalization of the logistic regression problem.\n",
    "\n",
    "The secret sauce to LDA is that it tries to find the best way to carve up your data into meaningful groups. On the left panel in the figure below, we see the distribution of some predictor X, conditional on the category of Y. On the right, we see the same distribution for random samples drawn from X.\n",
    "\n",
    "![LDA](imgs/L12Fig3.png)\n",
    "\n",
    "LDA asks, for any group $k$, what is the $P(Y=k|X)$? It does this by simply assuming that the groups are reflected by Gaussian distributions and uses conditional probability to find their means, variances, and (most importantly) the boundary that separates the two distributions.\n",
    "\n",
    "To work, the following have to be true about your data.\n",
    "\n",
    "**Assumptions**\n",
    "1.\tThe predictor variables are drawn from multivariate normal distributions.\n",
    "2.\tEqual variances of the underlying generative distributions. \n",
    "3.\tNo multicollinearity across variables in X. \n",
    "4.\tIndependence of errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Bayes theorem for classification:\n",
    "\n",
    "LDA specifically relies on **Bayes theorem**. \n",
    "\n",
    "$$ P(A|B) = \\frac{p(B|A)p(A)}{p(B)}$$\n",
    "\n",
    "* **likelihood:** $p(B|A)$\n",
    "* **prior:** $p(A)$\n",
    "    \n",
    "This is the fundamental structure of conditional probabilities. For example, using Bayes theorem we can ask what is the probability of it having rained if you see that the sidewalk is wet? This will be a function of how likely it is to rain in the first place, how likely it is to be wet if it had in fact rained, and how likely it is to be wet due to other reasons. \n",
    "\n",
    "Specifically,\n",
    "\n",
    "$$ P(rain|wet) = \\frac{p(wet|rain)p(rain)}{p(wet)}$$.\n",
    "\n",
    "Intuitively, the probability of it having rained, given that it is wet outside, increases if you live in a rainy country (UK) and decreases if you live in Israel where it rarely rains and wetness is more likely due to other reasons. It will also decrease if there are many likely reasons of it being wet (p(wet)) - if you live in 12th century, people used to empty latrines through the window on the street, which we don't do any more, but could cause the sidewalk to be wet without rain. Or the probability of it raining would increase if the rain was recent (p(wet|rain))\n",
    "\n",
    "LDA uses this conditional probability concept to estimate $k$ many probability functions, across X, for all $k$ groups in Y. In other words, _given any observation $x_i$, classify whether it belongs to an one of any K classes in Y._\n",
    "\n",
    "Because this is Bayesian, we also assume that $\\pi_k$ is the prior probability that a given observation is associated with the kth category. Thus, the approximate version of Bayes theorem as a classifer is \n",
    "\n",
    "$$P(Y = k|X=X) = \\frac{\\pi_kf_k(x)}{\\sum_{l=1}^K\\pi_lf_l(x)}$$\n",
    "\n",
    "This is the posterior probability for group $k$. If $P(Y = k|X=X)$ larger for one group than any other group, we say that there is a high probability that an observation in the $k^{th}$ class of Y. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An example with one predictor (p = 1):\n",
    "\n",
    "In order to see how this works, let's play with the functions a bit. \n",
    "\n",
    "As was mentioned above, here we assume that each observation $x_i$ from group $k$ is drawn from a Gaussian distribution. \n",
    "\n",
    "$$f_k(x_i) = \\frac{1}{\\sqrt{2\\pi}\\sigma_k}\\exp\\bigg(-\\frac{1}{2\\sigma^2_k}(x_i-\\mu_k)^2\\bigg)$$\n",
    "\n",
    "Now if we want to know whether that observation belongs in group $k$, versus any other group $l$, we can estimate it's probability by taking the ratio of the probability it belongs in $k$ against all other non-$k$ groups.\n",
    "\n",
    "$$p_k(x_i) = \\frac{\\pi_k\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\bigg(-\\frac{1}{2\\sigma^2}(x_i-\\mu_k)^2\\bigg)}{\\sum_{l=1}^{K}\\pi_l\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\bigg(-\\frac{1}{2\\sigma^2}(x_i-\\mu_l)^2\\bigg)}$$\n",
    "\n",
    "Notice that this is a bunch of messy exponentials. So let's work in the easier log-space.  $log(p_k(x))$ gives us the discriminant function, which we call $\\delta_k(x_i))$:\n",
    "\n",
    "\n",
    "$$\\delta_k(x_i) = x_i\\frac{\\mu_k}{\\sigma^2}-\\frac{\\mu_k^2}{2\\sigma^2}+\\log(\\pi_k)$$\n",
    "   \n",
    "Where $\\pi_k = \\frac{n_k}{n}$ (i.e., if you have more obsevations for one group than another, you have a higher prior probability of observations being in that group).\n",
    "\n",
    "The discriminant function $\\delta_k(x_i))$ is the the posterior probability estimate that x is in class K. Higher values of $\\delta_k(x_i)$ mean that the observation $x_i$ is more likely to belong in group $k$.\n",
    "\n",
    "What the discriminant function $\\delta_k(x_i))$ really does is determine what side of the _Bayes decision boundary_, the point that separates the two distributions, $x_i$ sits. In this case, the Bayes decision boundary is just the split distance between the means of the two distributions. \n",
    "\n",
    "$$x = \\frac{\\mu_1^2-\\mu_2^2}{2(\\mu_1-\\mu_2)} = \\frac{\\mu_1+\\mu_2}{2}$$\n",
    "\n",
    "This is because we are assuming that the distributions all have equal varianced.\n",
    "\n",
    "LDA tries to find best parameters of the underlying generative distributions for each group. These are,\n",
    "\n",
    "$$\\hat{\\mu}_k = \\frac{1}{n_k} \\sum_{i:y_i = k} x_i$$\n",
    "\n",
    "$$\\hat{\\sigma}^2 = \\frac{1}{n-K} \\sum_{k=1}^K \\sum_{i:y_i=k} (x_i - \\hat{\\mu}_k)^2 $$\n",
    "\n",
    "\n",
    "By plugging in the parameter estimates you get the discriminant function for each k category.\n",
    "\n",
    "$$\\hat{\\delta}_k(x) = x\\frac{\\hat{\\mu}_k}{\\hat{\\sigma}^2}-\\frac{\\hat{\\mu}_k^2}{2\\hat{\\sigma}^2}+\\log(\\hat{\\pi}_k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### The case with multiple predictors (p > 1)\n",
    "\n",
    "It is easy to extend the LDA approach to multiple predictors. In this case, we are using the multivariate GAussian density function:\n",
    "\n",
    "$$f(x) = \\frac{1}{\\sqrt{(2\\pi)^p|\\Sigma|}}\\exp\\bigg(-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)\\bigg)$$\n",
    "\n",
    "where $\\Sigma$ is the covariance matrix for all K classes.\n",
    "\n",
    "The discriminat function becomes:\n",
    "$$\\delta_k(x) = x^T\\Sigma^{-1}\\mu_k-\\frac{1}{2}\\mu_k^T\\Sigma^{-1}\\mu_k+log\\pi_k$$\n",
    "\n",
    "which is just a vectorized version of the discriminant function for p=1\n",
    "\n",
    "This results in the following decision boundaries in the example figure below:\n",
    "\n",
    "![LDA decision boundaries for p=2](imgs/L12fig5.png)\n",
    "\n",
    "Note that there are multiple decision boundaries (one for every pair of k categories). The dashed lines in the figure above show real Bayes decision boundaries (where the groups would separate if you knew the true underslying distributions), while the solid lines show the _estimated_ LDA decision boundary. As you can see, LDA does a really good job of approximating the Bayes optimal separation between groups.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Quadratic discriminant analysis (QDA) \n",
    "*(unsupervised method, parametric, classification)*\n",
    "\n",
    "A variant of LDA that is a bit more flexible is it's quadratic cousin, QDA. QDA differs from LDA in one critical way: it does not assume equal variances in your groups. \n",
    "\n",
    "Thus for QDA to work, your data only needs three properties.\n",
    "\n",
    "**Assumptions**\n",
    "1.\tThe predictor variables are drawn from multivariate normal distributions.\n",
    "2.\tNo multicollinearity across variables in X. \n",
    "3.\tIndependence of errors.\n",
    "\n",
    "Remember, LDA assumes a covariance matrix common to all $K$ classes. Notice that this QDA assumption is missing from above. QDA is an alternative version of LDA that does not assume homogeneity of variance. Instead, QDA assumes that each class has its own covariance matrix. In other words, the variability between classes does not have to be equal. \n",
    "\n",
    "Formally, it assumes that an observation $X = x$ from the $k$th class is $X ∼ N(\\mu_k,\\Sigma_k)$, where $\\Sigma_k$ is a covariance matrix for the $k$th class. This leads to one minor change in the form of the discrimant function where we also assume the covariance between factors and allow for different variances for all _k_ groups (not shown). This gives LDA *quadratic* properties. Hence the name.\n",
    "\n",
    "It is important to think about what this kind of flexibility means. Estimating a separate covariance matrix for each class increases the number of parameters to be estimated. The assumption of equal variance in LDA means that fewer parameters are estimated. So, LDA is more biased than QDA, and QDA is more flexible than LDA. \n",
    "\n",
    "Thus QDA is much more flexible than LDA, which can lead to a tendency to over-fit if you are not careful.\n",
    "\n",
    "To showcase the impact of the tradeoff between the two methods, see the figure below. This shows how both LDA and QDA perform when covariances are equal and when they are distinct. On the left, the two classes have a common covaraince. As a result, the Bayes decision boundary (purple and dashed) is linear and is well-approximated by the LDA decision boundary (black and dotted). Here, the QDA approximation of the decision boundary (green and solid) is poor, *because* of the flexibility (variance) of the estimate. On the right, the two classes have distinct variances between the $x$ variables (0.7 and -0.7). Now optimal/Bayes decision boundary is quadratic, and so QDA more accurately approximates the true boundary than LDA does, which assumes a linear boundary. \n",
    "\n",
    "<img src=\"imgs/lda_qda_bayes.png\" width=\"1000\">\n",
    "\n",
    "Remember to think carefully about the tradeoff between flexibility and bias for each method, and the implications of that tradeoff for interpreting the results of the analysis. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
